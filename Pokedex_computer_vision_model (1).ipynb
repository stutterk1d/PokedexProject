{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxKkhwmVO3Ph"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFPhu4vGXSij",
        "outputId": "3315dbc8-2238-41c0-fb5d-8faaf4782b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNucl5IPOspE",
        "outputId": "7a35cda3-2d0d-4c08-97d6-0e94d06d7404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5511 images belonging to 150 classes.\n",
            "Found 1309 images belonging to 150 classes.\n",
            "Found 1309 images belonging to 150 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/PokemonData'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generators for training, validation, and testing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Set the validation split\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "vXX-nT3EPhFQ",
        "outputId": "be950814-c9f3-452f-c08a-15bde63eaf04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ad0468aa9ced>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m     27\u001b[0m    \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        " # Create a CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(150, activation='softmax')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "model.save('pokemon_classifier.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnFDSSyC8iQ3",
        "outputId": "fd8377e6-3066-4620-8263-cc32e78d0e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7484375238418579\n"
          ]
        }
      ],
      "source": [
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_9uvjEPT0ls"
      },
      "source": [
        "Attempt #2 with using a VGG16 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlKTmc2RTz-x",
        "outputId": "b7d22b0f-ea8c-438d-f31b-5676ce386b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 72s 411ms/step - loss: 7.2342 - accuracy: 0.3716 - val_loss: 4.3731 - val_accuracy: 0.5891 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 71s 414ms/step - loss: 3.5114 - accuracy: 0.6704 - val_loss: 3.4300 - val_accuracy: 0.6023 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 69s 403ms/step - loss: 2.8839 - accuracy: 0.7176 - val_loss: 3.1480 - val_accuracy: 0.6602 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 71s 412ms/step - loss: 2.6976 - accuracy: 0.7281 - val_loss: 3.1232 - val_accuracy: 0.6078 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 70s 404ms/step - loss: 2.6298 - accuracy: 0.7324 - val_loss: 3.1043 - val_accuracy: 0.6195 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 69s 403ms/step - loss: 2.6412 - accuracy: 0.7260 - val_loss: 3.2134 - val_accuracy: 0.5844 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 2.6339 - accuracy: 0.7253 - val_loss: 3.0930 - val_accuracy: 0.6125 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 70s 409ms/step - loss: 2.5528 - accuracy: 0.7255 - val_loss: 3.3763 - val_accuracy: 0.5547 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 2.5962 - accuracy: 0.7098 - val_loss: 3.2127 - val_accuracy: 0.6023 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.5338 - accuracy: 0.7235\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 2.5338 - accuracy: 0.7235 - val_loss: 3.1947 - val_accuracy: 0.5813 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 2.1150 - accuracy: 0.8067 - val_loss: 2.1913 - val_accuracy: 0.7727 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 70s 405ms/step - loss: 1.7703 - accuracy: 0.8525 - val_loss: 1.9750 - val_accuracy: 0.7906 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 69s 399ms/step - loss: 1.5339 - accuracy: 0.8794 - val_loss: 1.7900 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 69s 404ms/step - loss: 1.3579 - accuracy: 0.8898 - val_loss: 1.6874 - val_accuracy: 0.7836 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 71s 412ms/step - loss: 1.2201 - accuracy: 0.9075 - val_loss: 1.5659 - val_accuracy: 0.7922 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 70s 410ms/step - loss: 1.0963 - accuracy: 0.9153 - val_loss: 1.5184 - val_accuracy: 0.7906 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 71s 412ms/step - loss: 0.9993 - accuracy: 0.9230 - val_loss: 1.4262 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 70s 405ms/step - loss: 0.9159 - accuracy: 0.9328 - val_loss: 1.3699 - val_accuracy: 0.7961 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 70s 409ms/step - loss: 0.8507 - accuracy: 0.9334 - val_loss: 1.3020 - val_accuracy: 0.8102 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.8011 - accuracy: 0.9431 - val_loss: 1.2870 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.7389 - accuracy: 0.9429 - val_loss: 1.2576 - val_accuracy: 0.7984 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "172/172 [==============================] - 71s 413ms/step - loss: 0.6926 - accuracy: 0.9511 - val_loss: 1.2146 - val_accuracy: 0.8086 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "172/172 [==============================] - 72s 416ms/step - loss: 0.6656 - accuracy: 0.9483 - val_loss: 1.2297 - val_accuracy: 0.8008 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "172/172 [==============================] - 73s 422ms/step - loss: 0.6382 - accuracy: 0.9518 - val_loss: 1.1948 - val_accuracy: 0.8008 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "172/172 [==============================] - 72s 416ms/step - loss: 0.6122 - accuracy: 0.9531 - val_loss: 1.2225 - val_accuracy: 0.7922 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "172/172 [==============================] - 72s 416ms/step - loss: 0.6020 - accuracy: 0.9533 - val_loss: 1.1782 - val_accuracy: 0.7992 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "172/172 [==============================] - 71s 413ms/step - loss: 0.5744 - accuracy: 0.9575 - val_loss: 1.1368 - val_accuracy: 0.8141 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "172/172 [==============================] - 71s 410ms/step - loss: 0.5608 - accuracy: 0.9588 - val_loss: 1.1556 - val_accuracy: 0.8000 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.5340 - accuracy: 0.9611 - val_loss: 1.1247 - val_accuracy: 0.7922 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "172/172 [==============================] - 70s 407ms/step - loss: 0.5219 - accuracy: 0.9615 - val_loss: 1.1455 - val_accuracy: 0.7922 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "172/172 [==============================] - 69s 403ms/step - loss: 0.4984 - accuracy: 0.9671 - val_loss: 1.1821 - val_accuracy: 0.7961 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.9609\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "172/172 [==============================] - 69s 404ms/step - loss: 0.5056 - accuracy: 0.9609 - val_loss: 1.1503 - val_accuracy: 0.7891 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "172/172 [==============================] - 69s 398ms/step - loss: 0.4701 - accuracy: 0.9686 - val_loss: 1.0764 - val_accuracy: 0.8094 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.4607 - accuracy: 0.9713 - val_loss: 1.0496 - val_accuracy: 0.8133 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "172/172 [==============================] - 71s 410ms/step - loss: 0.4532 - accuracy: 0.9728 - val_loss: 1.0361 - val_accuracy: 0.8148 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "172/172 [==============================] - 70s 405ms/step - loss: 0.4328 - accuracy: 0.9777 - val_loss: 1.0188 - val_accuracy: 0.8234 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.4261 - accuracy: 0.9765 - val_loss: 1.0064 - val_accuracy: 0.8305 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.4115 - accuracy: 0.9819 - val_loss: 0.9994 - val_accuracy: 0.8242 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.4103 - accuracy: 0.9823 - val_loss: 1.0029 - val_accuracy: 0.8195 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "172/172 [==============================] - 70s 405ms/step - loss: 0.4138 - accuracy: 0.9779 - val_loss: 0.9941 - val_accuracy: 0.8195 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "172/172 [==============================] - 69s 404ms/step - loss: 0.4005 - accuracy: 0.9827 - val_loss: 0.9728 - val_accuracy: 0.8297 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "172/172 [==============================] - 69s 401ms/step - loss: 0.3898 - accuracy: 0.9841 - val_loss: 0.9872 - val_accuracy: 0.8289 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "172/172 [==============================] - 70s 407ms/step - loss: 0.3812 - accuracy: 0.9849 - val_loss: 0.9891 - val_accuracy: 0.8180 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "172/172 [==============================] - 70s 410ms/step - loss: 0.3781 - accuracy: 0.9843 - val_loss: 0.9648 - val_accuracy: 0.8273 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "172/172 [==============================] - 71s 412ms/step - loss: 0.3754 - accuracy: 0.9859 - val_loss: 0.9797 - val_accuracy: 0.8172 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "172/172 [==============================] - 70s 405ms/step - loss: 0.3718 - accuracy: 0.9817 - val_loss: 1.0039 - val_accuracy: 0.8148 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "172/172 [==============================] - 70s 405ms/step - loss: 0.3627 - accuracy: 0.9869 - val_loss: 0.9565 - val_accuracy: 0.8352 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "172/172 [==============================] - 69s 400ms/step - loss: 0.3650 - accuracy: 0.9834 - val_loss: 0.9688 - val_accuracy: 0.8297 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.3545 - accuracy: 0.9838 - val_loss: 0.9560 - val_accuracy: 0.8289 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "172/172 [==============================] - 70s 404ms/step - loss: 0.3516 - accuracy: 0.9854 - val_loss: 0.9332 - val_accuracy: 0.8336 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "172/172 [==============================] - 69s 403ms/step - loss: 0.3494 - accuracy: 0.9865 - val_loss: 0.9644 - val_accuracy: 0.8203 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "172/172 [==============================] - 70s 407ms/step - loss: 0.3452 - accuracy: 0.9865 - val_loss: 0.9536 - val_accuracy: 0.8375 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9881\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.3410 - accuracy: 0.9881 - val_loss: 0.9367 - val_accuracy: 0.8313 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "172/172 [==============================] - 69s 401ms/step - loss: 0.3399 - accuracy: 0.9859 - val_loss: 0.9440 - val_accuracy: 0.8383 - lr: 1.0000e-06\n",
            "Epoch 55/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.3414 - accuracy: 0.9854 - val_loss: 0.9207 - val_accuracy: 0.8305 - lr: 1.0000e-06\n",
            "Epoch 56/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.3384 - accuracy: 0.9861 - val_loss: 0.9743 - val_accuracy: 0.8148 - lr: 1.0000e-06\n",
            "Epoch 57/100\n",
            "172/172 [==============================] - 71s 411ms/step - loss: 0.3313 - accuracy: 0.9869 - val_loss: 0.9413 - val_accuracy: 0.8266 - lr: 1.0000e-06\n",
            "Epoch 58/100\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.9883\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.3302 - accuracy: 0.9883 - val_loss: 0.9442 - val_accuracy: 0.8320 - lr: 1.0000e-06\n",
            "Epoch 59/100\n",
            "172/172 [==============================] - 70s 406ms/step - loss: 0.3259 - accuracy: 0.9912 - val_loss: 0.9582 - val_accuracy: 0.8273 - lr: 1.0000e-07\n",
            "Epoch 60/100\n",
            "172/172 [==============================] - 70s 408ms/step - loss: 0.3339 - accuracy: 0.9890 - val_loss: 0.9497 - val_accuracy: 0.8242 - lr: 1.0000e-07\n",
            "40/40 [==============================] - 13s 330ms/step - loss: 0.9420 - accuracy: 0.8367\n",
            "Test accuracy: 0.836718738079071\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(150, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "# Save the model\n",
        "model.save('pokemon_classifier2.0.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XcBLAw30pQo"
      },
      "source": [
        "Attempt #3 with changed parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLmpOW4a6yhs",
        "outputId": "7e2c83f8-fc72-43f3-d04b-74b141704487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5511 images belonging to 150 classes.\n",
            "Found 1309 images belonging to 150 classes.\n",
            "Found 6820 images belonging to 150 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "172/172 [==============================] - 4770s 28s/step - loss: 12.6891 - accuracy: 0.0113 - val_loss: 8.7344 - val_accuracy: 0.0227 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 8.1612 - accuracy: 0.0124 - val_loss: 8.2573 - val_accuracy: 0.0086 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 6.7885 - accuracy: 0.0173 - val_loss: 8.9754 - val_accuracy: 0.0055 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "172/172 [==============================] - 65s 381ms/step - loss: 6.1043 - accuracy: 0.0214 - val_loss: 7.5873 - val_accuracy: 0.0102 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 5.8897 - accuracy: 0.0184 - val_loss: 6.4643 - val_accuracy: 0.0305 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 5.7507 - accuracy: 0.0257 - val_loss: 9538.3213 - val_accuracy: 0.0078 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "172/172 [==============================] - 67s 388ms/step - loss: 5.7693 - accuracy: 0.0279 - val_loss: 6.5538 - val_accuracy: 0.0258 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 5.7022 - accuracy: 0.0396 - val_loss: 7.0196 - val_accuracy: 0.0227 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 5.8174 - accuracy: 0.0480 - val_loss: 6.8965 - val_accuracy: 0.0344 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 5.7838 - accuracy: 0.0558 - val_loss: 5.4842 - val_accuracy: 0.0766 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 5.4909 - accuracy: 0.0739 - val_loss: 7.1796 - val_accuracy: 0.0273 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 5.3999 - accuracy: 0.0814 - val_loss: 13.0731 - val_accuracy: 0.0164 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "172/172 [==============================] - 66s 382ms/step - loss: 5.5421 - accuracy: 0.0894 - val_loss: 5.6513 - val_accuracy: 0.0789 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 5.0715 - accuracy: 0.1274 - val_loss: 5.2644 - val_accuracy: 0.1102 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 4.7955 - accuracy: 0.1601 - val_loss: 4.8523 - val_accuracy: 0.1406 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 5.0243 - accuracy: 0.1462 - val_loss: 7.3118 - val_accuracy: 0.0484 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 4.7698 - accuracy: 0.1522 - val_loss: 7.5561 - val_accuracy: 0.0562 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 4.3181 - accuracy: 0.1968 - val_loss: 4.7582 - val_accuracy: 0.1289 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 4.0288 - accuracy: 0.2245 - val_loss: 4.0491 - val_accuracy: 0.2320 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 3.8042 - accuracy: 0.2639 - val_loss: 5.3503 - val_accuracy: 0.1125 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 3.5951 - accuracy: 0.3017 - val_loss: 18.2070 - val_accuracy: 0.0328 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "172/172 [==============================] - 68s 395ms/step - loss: 3.4958 - accuracy: 0.3141 - val_loss: 3.8999 - val_accuracy: 0.2438 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "172/172 [==============================] - 67s 387ms/step - loss: 3.3746 - accuracy: 0.3335 - val_loss: 3.5796 - val_accuracy: 0.3125 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 3.2292 - accuracy: 0.3605 - val_loss: 3.1226 - val_accuracy: 0.4000 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 3.0304 - accuracy: 0.3986 - val_loss: 3.2857 - val_accuracy: 0.3555 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "172/172 [==============================] - 66s 382ms/step - loss: 3.0510 - accuracy: 0.3944 - val_loss: 3.2661 - val_accuracy: 0.3602 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "172/172 [==============================] - 67s 387ms/step - loss: 2.8298 - accuracy: 0.4442 - val_loss: 2.8321 - val_accuracy: 0.4531 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "172/172 [==============================] - 67s 387ms/step - loss: 3.3355 - accuracy: 0.3535 - val_loss: 5.9021 - val_accuracy: 0.1445 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 3.1657 - accuracy: 0.3864 - val_loss: 4.0594 - val_accuracy: 0.2734 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "172/172 [==============================] - 64s 375ms/step - loss: 3.2631 - accuracy: 0.3734 - val_loss: 6.6395 - val_accuracy: 0.1086 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 2.8267 - accuracy: 0.4557 - val_loss: 193.7897 - val_accuracy: 0.0078 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.6592 - accuracy: 0.4630\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 2.6592 - accuracy: 0.4630 - val_loss: 3.4249 - val_accuracy: 0.3453 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 2.3771 - accuracy: 0.5291 - val_loss: 2.2586 - val_accuracy: 0.5914 - lr: 1.0000e-04\n",
            "Epoch 34/1000\n",
            "172/172 [==============================] - 63s 368ms/step - loss: 2.2208 - accuracy: 0.5813 - val_loss: 2.1304 - val_accuracy: 0.6250 - lr: 1.0000e-04\n",
            "Epoch 35/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 2.2578 - accuracy: 0.5598 - val_loss: 2.1885 - val_accuracy: 0.5906 - lr: 1.0000e-04\n",
            "Epoch 36/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 2.1277 - accuracy: 0.5784 - val_loss: 2.0418 - val_accuracy: 0.6172 - lr: 1.0000e-04\n",
            "Epoch 37/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 2.0717 - accuracy: 0.5813 - val_loss: 2.0044 - val_accuracy: 0.6172 - lr: 1.0000e-04\n",
            "Epoch 38/1000\n",
            "172/172 [==============================] - 64s 370ms/step - loss: 2.0220 - accuracy: 0.5939 - val_loss: 1.9713 - val_accuracy: 0.6297 - lr: 1.0000e-04\n",
            "Epoch 39/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 1.9904 - accuracy: 0.5903 - val_loss: 1.9656 - val_accuracy: 0.6258 - lr: 1.0000e-04\n",
            "Epoch 40/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 1.9375 - accuracy: 0.5976 - val_loss: 1.9136 - val_accuracy: 0.6242 - lr: 1.0000e-04\n",
            "Epoch 41/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 1.9104 - accuracy: 0.6076 - val_loss: 1.8651 - val_accuracy: 0.6508 - lr: 1.0000e-04\n",
            "Epoch 42/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 1.9196 - accuracy: 0.5968 - val_loss: 1.9134 - val_accuracy: 0.6187 - lr: 1.0000e-04\n",
            "Epoch 43/1000\n",
            "172/172 [==============================] - 66s 387ms/step - loss: 1.8651 - accuracy: 0.6041 - val_loss: 1.8710 - val_accuracy: 0.6227 - lr: 1.0000e-04\n",
            "Epoch 44/1000\n",
            "172/172 [==============================] - 68s 394ms/step - loss: 1.8050 - accuracy: 0.6237 - val_loss: 1.8156 - val_accuracy: 0.6359 - lr: 1.0000e-04\n",
            "Epoch 45/1000\n",
            "172/172 [==============================] - 67s 386ms/step - loss: 1.7729 - accuracy: 0.6249 - val_loss: 1.7652 - val_accuracy: 0.6484 - lr: 1.0000e-04\n",
            "Epoch 46/1000\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 1.7671 - accuracy: 0.6191 - val_loss: 1.7448 - val_accuracy: 0.6492 - lr: 1.0000e-04\n",
            "Epoch 47/1000\n",
            "172/172 [==============================] - 64s 375ms/step - loss: 1.7219 - accuracy: 0.6377 - val_loss: 1.7698 - val_accuracy: 0.6336 - lr: 1.0000e-04\n",
            "Epoch 48/1000\n",
            "172/172 [==============================] - 64s 375ms/step - loss: 1.7040 - accuracy: 0.6300 - val_loss: 1.7853 - val_accuracy: 0.6469 - lr: 1.0000e-04\n",
            "Epoch 49/1000\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 1.6595 - accuracy: 0.6397 - val_loss: 1.6919 - val_accuracy: 0.6539 - lr: 1.0000e-04\n",
            "Epoch 50/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 1.6416 - accuracy: 0.6412 - val_loss: 1.6862 - val_accuracy: 0.6703 - lr: 1.0000e-04\n",
            "Epoch 51/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.5846 - accuracy: 0.6481 - val_loss: 1.6659 - val_accuracy: 0.6492 - lr: 1.0000e-04\n",
            "Epoch 52/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.6035 - accuracy: 0.6412 - val_loss: 1.6947 - val_accuracy: 0.6297 - lr: 1.0000e-04\n",
            "Epoch 53/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.5786 - accuracy: 0.6454 - val_loss: 1.6690 - val_accuracy: 0.6336 - lr: 1.0000e-04\n",
            "Epoch 54/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 1.5714 - accuracy: 0.6461 - val_loss: 1.6272 - val_accuracy: 0.6633 - lr: 1.0000e-04\n",
            "Epoch 55/1000\n",
            "172/172 [==============================] - 66s 382ms/step - loss: 1.5290 - accuracy: 0.6448 - val_loss: 1.6379 - val_accuracy: 0.6516 - lr: 1.0000e-04\n",
            "Epoch 56/1000\n",
            "172/172 [==============================] - 64s 370ms/step - loss: 1.5406 - accuracy: 0.6392 - val_loss: 1.5710 - val_accuracy: 0.6586 - lr: 1.0000e-04\n",
            "Epoch 57/1000\n",
            "172/172 [==============================] - 63s 365ms/step - loss: 1.4674 - accuracy: 0.6664 - val_loss: 1.5970 - val_accuracy: 0.6750 - lr: 1.0000e-04\n",
            "Epoch 58/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 1.4488 - accuracy: 0.6665 - val_loss: 1.5624 - val_accuracy: 0.6656 - lr: 1.0000e-04\n",
            "Epoch 59/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 1.4191 - accuracy: 0.6676 - val_loss: 1.5342 - val_accuracy: 0.6680 - lr: 1.0000e-04\n",
            "Epoch 60/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.4229 - accuracy: 0.6698 - val_loss: 1.5297 - val_accuracy: 0.6695 - lr: 1.0000e-04\n",
            "Epoch 61/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 1.3843 - accuracy: 0.6742 - val_loss: 1.5109 - val_accuracy: 0.6766 - lr: 1.0000e-04\n",
            "Epoch 62/1000\n",
            "172/172 [==============================] - 63s 369ms/step - loss: 1.3868 - accuracy: 0.6698 - val_loss: 1.5721 - val_accuracy: 0.6516 - lr: 1.0000e-04\n",
            "Epoch 63/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 1.3777 - accuracy: 0.6808 - val_loss: 1.6345 - val_accuracy: 0.6281 - lr: 1.0000e-04\n",
            "Epoch 64/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 1.3445 - accuracy: 0.6698 - val_loss: 1.4717 - val_accuracy: 0.6820 - lr: 1.0000e-04\n",
            "Epoch 65/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.2972 - accuracy: 0.6906 - val_loss: 1.4013 - val_accuracy: 0.6820 - lr: 1.0000e-04\n",
            "Epoch 66/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.2937 - accuracy: 0.6894 - val_loss: 1.4581 - val_accuracy: 0.6664 - lr: 1.0000e-04\n",
            "Epoch 67/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 1.3017 - accuracy: 0.6806 - val_loss: 1.3971 - val_accuracy: 0.6891 - lr: 1.0000e-04\n",
            "Epoch 68/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.2660 - accuracy: 0.6892 - val_loss: 1.4628 - val_accuracy: 0.6469 - lr: 1.0000e-04\n",
            "Epoch 69/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 1.2441 - accuracy: 0.6990 - val_loss: 1.3816 - val_accuracy: 0.6797 - lr: 1.0000e-04\n",
            "Epoch 70/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 1.2557 - accuracy: 0.6894 - val_loss: 1.4230 - val_accuracy: 0.6781 - lr: 1.0000e-04\n",
            "Epoch 71/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 1.2227 - accuracy: 0.6967 - val_loss: 1.5973 - val_accuracy: 0.6187 - lr: 1.0000e-04\n",
            "Epoch 72/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 1.2029 - accuracy: 0.7007 - val_loss: 1.4019 - val_accuracy: 0.6922 - lr: 1.0000e-04\n",
            "Epoch 73/1000\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 1.1931 - accuracy: 0.7069 - val_loss: 1.4696 - val_accuracy: 0.6438 - lr: 1.0000e-04\n",
            "Epoch 74/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.1799 - accuracy: 0.7102\n",
            "Epoch 74: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.1799 - accuracy: 0.7102 - val_loss: 4.0504 - val_accuracy: 0.2906 - lr: 1.0000e-04\n",
            "Epoch 75/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 1.2129 - accuracy: 0.7016 - val_loss: 1.3702 - val_accuracy: 0.6836 - lr: 1.0000e-05\n",
            "Epoch 76/1000\n",
            "172/172 [==============================] - 64s 374ms/step - loss: 1.1978 - accuracy: 0.7062 - val_loss: 1.3006 - val_accuracy: 0.7102 - lr: 1.0000e-05\n",
            "Epoch 77/1000\n",
            "172/172 [==============================] - 64s 369ms/step - loss: 1.1430 - accuracy: 0.7211 - val_loss: 1.2697 - val_accuracy: 0.7016 - lr: 1.0000e-05\n",
            "Epoch 78/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 1.1200 - accuracy: 0.7275 - val_loss: 1.2805 - val_accuracy: 0.6992 - lr: 1.0000e-05\n",
            "Epoch 79/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.1255 - accuracy: 0.7197 - val_loss: 1.3249 - val_accuracy: 0.6992 - lr: 1.0000e-05\n",
            "Epoch 80/1000\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 1.1365 - accuracy: 0.7166 - val_loss: 1.2795 - val_accuracy: 0.7047 - lr: 1.0000e-05\n",
            "Epoch 81/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 1.1023 - accuracy: 0.7301 - val_loss: 1.3001 - val_accuracy: 0.6922 - lr: 1.0000e-05\n",
            "Epoch 82/1000\n",
            "172/172 [==============================] - 66s 385ms/step - loss: 1.1170 - accuracy: 0.7224 - val_loss: 1.2490 - val_accuracy: 0.7172 - lr: 1.0000e-05\n",
            "Epoch 83/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 1.0949 - accuracy: 0.7333 - val_loss: 1.2752 - val_accuracy: 0.7063 - lr: 1.0000e-05\n",
            "Epoch 84/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 1.0993 - accuracy: 0.7246 - val_loss: 1.2561 - val_accuracy: 0.7078 - lr: 1.0000e-05\n",
            "Epoch 85/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 1.0980 - accuracy: 0.7297 - val_loss: 1.2890 - val_accuracy: 0.7008 - lr: 1.0000e-05\n",
            "Epoch 86/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.0837 - accuracy: 0.7346 - val_loss: 1.2584 - val_accuracy: 0.6961 - lr: 1.0000e-05\n",
            "Epoch 87/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.0795 - accuracy: 0.7310\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "172/172 [==============================] - 66s 384ms/step - loss: 1.0795 - accuracy: 0.7310 - val_loss: 1.3108 - val_accuracy: 0.7086 - lr: 1.0000e-05\n",
            "Epoch 88/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.0902 - accuracy: 0.7282 - val_loss: 1.2931 - val_accuracy: 0.7055 - lr: 1.0000e-06\n",
            "Epoch 89/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.0647 - accuracy: 0.7385 - val_loss: 1.2497 - val_accuracy: 0.7195 - lr: 1.0000e-06\n",
            "Epoch 90/1000\n",
            "172/172 [==============================] - 65s 374ms/step - loss: 1.0848 - accuracy: 0.7282 - val_loss: 1.2530 - val_accuracy: 0.7023 - lr: 1.0000e-06\n",
            "Epoch 91/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 1.0873 - accuracy: 0.7239 - val_loss: 1.2623 - val_accuracy: 0.7023 - lr: 1.0000e-06\n",
            "Epoch 92/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.0905 - accuracy: 0.7255\n",
            "Epoch 92: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "172/172 [==============================] - 67s 389ms/step - loss: 1.0905 - accuracy: 0.7255 - val_loss: 1.2876 - val_accuracy: 0.7070 - lr: 1.0000e-06\n",
            "214/214 [==============================] - 35s 162ms/step - loss: 0.6298 - accuracy: 0.8823\n",
            "Test accuracy: 0.8822580575942993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/PokemonData'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generators for training, validation, and testing with aggressive data augmentation\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Ensure data isn't shuffled for evaluation\n",
        ")\n",
        "# Load VGG16 base model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Fine-tune the last few layers of the base model\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build the model with additional layers and regularization\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Dense(150, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping and learning rate reduction callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "\n",
        "# Train the model with increased epochs\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=1000,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "try:\n",
        "    test_loss, test_acc = model.evaluate(test_generator)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "except ValueError as e:\n",
        "    print(\"Error evaluating the model on the test set:\", e)\n",
        "\n",
        "# Save the model\n",
        "model.save('pokemon_classifier3.0_improved.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempt #4 changing parameters again!"
      ],
      "metadata": {
        "id": "ajMnEnIVrabB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCq6L-FK63MB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917392f4-d4b9-4aa3-cd7a-5a8833029063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5511 images belonging to 150 classes.\n",
            "Found 1309 images belonging to 150 classes.\n",
            "Found 6820 images belonging to 150 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "172/172 [==============================] - 67s 375ms/step - loss: 5.5907 - accuracy: 0.0151 - val_loss: 5.1893 - val_accuracy: 0.0164 - lr: 0.0010\n",
            "Epoch 2/1000\n",
            "172/172 [==============================] - 65s 374ms/step - loss: 4.9913 - accuracy: 0.0316 - val_loss: 4.8733 - val_accuracy: 0.0437 - lr: 0.0010\n",
            "Epoch 3/1000\n",
            "172/172 [==============================] - 66s 382ms/step - loss: 4.6466 - accuracy: 0.0489 - val_loss: 13.2918 - val_accuracy: 0.0125 - lr: 0.0010\n",
            "Epoch 4/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 4.2176 - accuracy: 0.0730 - val_loss: 5.9835 - val_accuracy: 0.0398 - lr: 0.0010\n",
            "Epoch 5/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 3.9424 - accuracy: 0.0976 - val_loss: 5.2378 - val_accuracy: 0.0430 - lr: 0.0010\n",
            "Epoch 6/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 3.6822 - accuracy: 0.1267 - val_loss: 4.9565 - val_accuracy: 0.0719 - lr: 0.0010\n",
            "Epoch 7/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 3.5535 - accuracy: 0.1380 - val_loss: 4.2095 - val_accuracy: 0.1320 - lr: 0.0010\n",
            "Epoch 8/1000\n",
            "172/172 [==============================] - 66s 383ms/step - loss: 3.3275 - accuracy: 0.1683 - val_loss: 3.1192 - val_accuracy: 0.2375 - lr: 0.0010\n",
            "Epoch 9/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 3.1321 - accuracy: 0.2075 - val_loss: 3.5604 - val_accuracy: 0.1906 - lr: 0.0010\n",
            "Epoch 10/1000\n",
            "172/172 [==============================] - 63s 364ms/step - loss: 3.0796 - accuracy: 0.2197 - val_loss: 4.6599 - val_accuracy: 0.1258 - lr: 0.0010\n",
            "Epoch 11/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 2.8116 - accuracy: 0.2668 - val_loss: 2.8453 - val_accuracy: 0.3367 - lr: 0.0010\n",
            "Epoch 12/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 2.5732 - accuracy: 0.3163 - val_loss: 2.7220 - val_accuracy: 0.3016 - lr: 0.0010\n",
            "Epoch 13/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 2.4309 - accuracy: 0.3409 - val_loss: 3.4616 - val_accuracy: 0.2211 - lr: 0.0010\n",
            "Epoch 14/1000\n",
            "172/172 [==============================] - 64s 374ms/step - loss: 2.6845 - accuracy: 0.2979 - val_loss: 4.1462 - val_accuracy: 0.1891 - lr: 0.0010\n",
            "Epoch 15/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 2.3598 - accuracy: 0.3608 - val_loss: 3.1929 - val_accuracy: 0.2773 - lr: 0.0010\n",
            "Epoch 16/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 2.1170 - accuracy: 0.4176 - val_loss: 3.0600 - val_accuracy: 0.2820 - lr: 0.0010\n",
            "Epoch 17/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 2.0344 - accuracy: 0.4318 - val_loss: 3.1450 - val_accuracy: 0.2625 - lr: 0.0010\n",
            "Epoch 18/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 1.9037 - accuracy: 0.4674 - val_loss: 2.6493 - val_accuracy: 0.3367 - lr: 0.0010\n",
            "Epoch 19/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.7739 - accuracy: 0.4955 - val_loss: 3.8078 - val_accuracy: 0.4945 - lr: 0.0010\n",
            "Epoch 20/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 1.6681 - accuracy: 0.5187 - val_loss: 2.2047 - val_accuracy: 0.5164 - lr: 0.0010\n",
            "Epoch 21/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.5558 - accuracy: 0.5532 - val_loss: 3.5684 - val_accuracy: 0.5328 - lr: 0.0010\n",
            "Epoch 22/1000\n",
            "172/172 [==============================] - 64s 374ms/step - loss: 1.4734 - accuracy: 0.5720 - val_loss: 2.1732 - val_accuracy: 0.4555 - lr: 0.0010\n",
            "Epoch 23/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 1.7755 - accuracy: 0.5205 - val_loss: 6.3677 - val_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 24/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.6012 - accuracy: 0.5421 - val_loss: 2.0103 - val_accuracy: 0.4859 - lr: 0.0010\n",
            "Epoch 25/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 1.3720 - accuracy: 0.6018 - val_loss: 1.9185 - val_accuracy: 0.5609 - lr: 0.0010\n",
            "Epoch 26/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 1.2589 - accuracy: 0.6333 - val_loss: 1.3545 - val_accuracy: 0.6422 - lr: 0.0010\n",
            "Epoch 27/1000\n",
            "172/172 [==============================] - 63s 367ms/step - loss: 1.1765 - accuracy: 0.6631 - val_loss: 2.0614 - val_accuracy: 0.5734 - lr: 0.0010\n",
            "Epoch 28/1000\n",
            "172/172 [==============================] - 62s 363ms/step - loss: 1.1552 - accuracy: 0.6565 - val_loss: 1.8898 - val_accuracy: 0.5930 - lr: 0.0010\n",
            "Epoch 29/1000\n",
            "172/172 [==============================] - 62s 362ms/step - loss: 1.1167 - accuracy: 0.6700 - val_loss: 1.9013 - val_accuracy: 0.6109 - lr: 0.0010\n",
            "Epoch 30/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 1.0656 - accuracy: 0.6844 - val_loss: 1.6848 - val_accuracy: 0.6438 - lr: 0.0010\n",
            "Epoch 31/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 0.9997 - accuracy: 0.7041 - val_loss: 1.4077 - val_accuracy: 0.6398 - lr: 0.0010\n",
            "Epoch 32/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 0.9412 - accuracy: 0.7167 - val_loss: 1.3837 - val_accuracy: 0.6516 - lr: 0.0010\n",
            "Epoch 33/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 0.9079 - accuracy: 0.7335 - val_loss: 1.3350 - val_accuracy: 0.6453 - lr: 0.0010\n",
            "Epoch 34/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 0.8813 - accuracy: 0.7361 - val_loss: 2.4066 - val_accuracy: 0.6461 - lr: 0.0010\n",
            "Epoch 35/1000\n",
            "172/172 [==============================] - 63s 368ms/step - loss: 0.8461 - accuracy: 0.7459 - val_loss: 2.4231 - val_accuracy: 0.6750 - lr: 0.0010\n",
            "Epoch 36/1000\n",
            "172/172 [==============================] - 63s 368ms/step - loss: 0.7889 - accuracy: 0.7565 - val_loss: 1.5886 - val_accuracy: 0.6930 - lr: 0.0010\n",
            "Epoch 37/1000\n",
            "172/172 [==============================] - 63s 364ms/step - loss: 0.7765 - accuracy: 0.7618 - val_loss: 2.2138 - val_accuracy: 0.6766 - lr: 0.0010\n",
            "Epoch 38/1000\n",
            "172/172 [==============================] - 62s 362ms/step - loss: 0.7186 - accuracy: 0.7876 - val_loss: 2.0066 - val_accuracy: 0.6633 - lr: 0.0010\n",
            "Epoch 39/1000\n",
            "172/172 [==============================] - 62s 363ms/step - loss: 0.7242 - accuracy: 0.7830 - val_loss: 2.3115 - val_accuracy: 0.6945 - lr: 0.0010\n",
            "Epoch 40/1000\n",
            "172/172 [==============================] - 63s 367ms/step - loss: 0.6661 - accuracy: 0.7896 - val_loss: 1.4040 - val_accuracy: 0.7133 - lr: 0.0010\n",
            "Epoch 41/1000\n",
            "172/172 [==============================] - 62s 362ms/step - loss: 0.6626 - accuracy: 0.7996 - val_loss: 1.1689 - val_accuracy: 0.7023 - lr: 0.0010\n",
            "Epoch 42/1000\n",
            "172/172 [==============================] - 63s 366ms/step - loss: 0.6364 - accuracy: 0.8027 - val_loss: 2.0744 - val_accuracy: 0.7156 - lr: 0.0010\n",
            "Epoch 43/1000\n",
            "172/172 [==============================] - 63s 363ms/step - loss: 0.6308 - accuracy: 0.8091 - val_loss: 1.8575 - val_accuracy: 0.7039 - lr: 0.0010\n",
            "Epoch 44/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 0.6023 - accuracy: 0.8109 - val_loss: 1.0766 - val_accuracy: 0.7461 - lr: 0.0010\n",
            "Epoch 45/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.5579 - accuracy: 0.8317 - val_loss: 2.4798 - val_accuracy: 0.7141 - lr: 0.0010\n",
            "Epoch 46/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 0.5344 - accuracy: 0.8337 - val_loss: 1.7127 - val_accuracy: 0.6414 - lr: 0.0010\n",
            "Epoch 47/1000\n",
            "172/172 [==============================] - 63s 368ms/step - loss: 0.5204 - accuracy: 0.8363 - val_loss: 1.1968 - val_accuracy: 0.7398 - lr: 0.0010\n",
            "Epoch 48/1000\n",
            "172/172 [==============================] - 63s 366ms/step - loss: 0.7083 - accuracy: 0.7861 - val_loss: 1.5477 - val_accuracy: 0.7336 - lr: 0.0010\n",
            "Epoch 49/1000\n",
            "172/172 [==============================] - 64s 374ms/step - loss: 0.5578 - accuracy: 0.8306 - val_loss: 1.6159 - val_accuracy: 0.7250 - lr: 0.0010\n",
            "Epoch 50/1000\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 0.4600 - accuracy: 0.8593 - val_loss: 0.9634 - val_accuracy: 0.7719 - lr: 0.0010\n",
            "Epoch 51/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 0.4814 - accuracy: 0.8522 - val_loss: 1.3083 - val_accuracy: 0.7148 - lr: 0.0010\n",
            "Epoch 52/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 0.4759 - accuracy: 0.8544 - val_loss: 1.6330 - val_accuracy: 0.7484 - lr: 0.0010\n",
            "Epoch 53/1000\n",
            "172/172 [==============================] - 63s 364ms/step - loss: 0.4536 - accuracy: 0.8567 - val_loss: 1.3458 - val_accuracy: 0.7156 - lr: 0.0010\n",
            "Epoch 54/1000\n",
            "172/172 [==============================] - 63s 366ms/step - loss: 0.4458 - accuracy: 0.8617 - val_loss: 1.6974 - val_accuracy: 0.7758 - lr: 0.0010\n",
            "Epoch 55/1000\n",
            "172/172 [==============================] - 64s 374ms/step - loss: 0.4239 - accuracy: 0.8662 - val_loss: 1.3037 - val_accuracy: 0.7594 - lr: 0.0010\n",
            "Epoch 56/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 0.5187 - accuracy: 0.8474 - val_loss: 1.9117 - val_accuracy: 0.7656 - lr: 0.0010\n",
            "Epoch 57/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 0.4175 - accuracy: 0.8666 - val_loss: 3.3675 - val_accuracy: 0.7695 - lr: 0.0010\n",
            "Epoch 58/1000\n",
            "172/172 [==============================] - 64s 374ms/step - loss: 0.3922 - accuracy: 0.8753 - val_loss: 0.8807 - val_accuracy: 0.7930 - lr: 0.0010\n",
            "Epoch 59/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 0.3420 - accuracy: 0.8923 - val_loss: 0.9302 - val_accuracy: 0.7789 - lr: 0.0010\n",
            "Epoch 60/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 0.3590 - accuracy: 0.8936 - val_loss: 0.9625 - val_accuracy: 0.7711 - lr: 0.0010\n",
            "Epoch 61/1000\n",
            "172/172 [==============================] - 64s 372ms/step - loss: 0.3608 - accuracy: 0.8885 - val_loss: 1.9787 - val_accuracy: 0.7641 - lr: 0.0010\n",
            "Epoch 62/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 0.3282 - accuracy: 0.9007 - val_loss: 4.6575 - val_accuracy: 0.7234 - lr: 0.0010\n",
            "Epoch 63/1000\n",
            "172/172 [==============================] - 64s 373ms/step - loss: 0.3390 - accuracy: 0.8956 - val_loss: 1.5072 - val_accuracy: 0.7812 - lr: 0.0010\n",
            "Epoch 64/1000\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 0.3726 - accuracy: 0.8843 - val_loss: 2.6786 - val_accuracy: 0.7695 - lr: 0.0010\n",
            "Epoch 65/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.3363 - accuracy: 0.8947 - val_loss: 1.0789 - val_accuracy: 0.7641 - lr: 0.0010\n",
            "Epoch 66/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 0.3054 - accuracy: 0.9073 - val_loss: 1.3980 - val_accuracy: 0.7844 - lr: 0.0010\n",
            "Epoch 67/1000\n",
            "172/172 [==============================] - 66s 387ms/step - loss: 0.3361 - accuracy: 0.9009 - val_loss: 1.0050 - val_accuracy: 0.7812 - lr: 0.0010\n",
            "Epoch 68/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.9036\n",
            "Epoch 68: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "172/172 [==============================] - 67s 387ms/step - loss: 0.3091 - accuracy: 0.9036 - val_loss: 1.3756 - val_accuracy: 0.7375 - lr: 0.0010\n",
            "Epoch 69/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.2215 - accuracy: 0.9286 - val_loss: 0.7345 - val_accuracy: 0.8281 - lr: 2.0000e-04\n",
            "Epoch 70/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.1613 - accuracy: 0.9494 - val_loss: 0.9276 - val_accuracy: 0.8180 - lr: 2.0000e-04\n",
            "Epoch 71/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.1663 - accuracy: 0.9491 - val_loss: 0.9036 - val_accuracy: 0.8305 - lr: 2.0000e-04\n",
            "Epoch 72/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 0.1580 - accuracy: 0.9544 - val_loss: 0.8041 - val_accuracy: 0.8266 - lr: 2.0000e-04\n",
            "Epoch 73/1000\n",
            "172/172 [==============================] - 67s 391ms/step - loss: 0.1364 - accuracy: 0.9593 - val_loss: 1.0542 - val_accuracy: 0.8289 - lr: 2.0000e-04\n",
            "Epoch 74/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.1348 - accuracy: 0.9584 - val_loss: 0.8562 - val_accuracy: 0.8281 - lr: 2.0000e-04\n",
            "Epoch 75/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.1353 - accuracy: 0.9560 - val_loss: 0.7800 - val_accuracy: 0.8289 - lr: 2.0000e-04\n",
            "Epoch 76/1000\n",
            "172/172 [==============================] - 63s 366ms/step - loss: 0.1425 - accuracy: 0.9577 - val_loss: 0.8201 - val_accuracy: 0.8242 - lr: 2.0000e-04\n",
            "Epoch 77/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 0.1361 - accuracy: 0.9536 - val_loss: 1.0900 - val_accuracy: 0.8289 - lr: 2.0000e-04\n",
            "Epoch 78/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 0.1380 - accuracy: 0.9609 - val_loss: 3.9945 - val_accuracy: 0.8250 - lr: 2.0000e-04\n",
            "Epoch 79/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9524\n",
            "Epoch 79: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 0.1451 - accuracy: 0.9524 - val_loss: 1.5257 - val_accuracy: 0.8297 - lr: 2.0000e-04\n",
            "Epoch 80/1000\n",
            "172/172 [==============================] - 65s 378ms/step - loss: 0.1212 - accuracy: 0.9613 - val_loss: 1.4641 - val_accuracy: 0.8266 - lr: 4.0000e-05\n",
            "Epoch 81/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 0.1223 - accuracy: 0.9628 - val_loss: 0.7855 - val_accuracy: 0.8227 - lr: 4.0000e-05\n",
            "Epoch 82/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 0.1242 - accuracy: 0.9642 - val_loss: 1.5197 - val_accuracy: 0.8367 - lr: 4.0000e-05\n",
            "Epoch 83/1000\n",
            "172/172 [==============================] - 64s 370ms/step - loss: 0.1187 - accuracy: 0.9637 - val_loss: 0.9039 - val_accuracy: 0.8422 - lr: 4.0000e-05\n",
            "Epoch 84/1000\n",
            "172/172 [==============================] - 63s 364ms/step - loss: 0.1145 - accuracy: 0.9657 - val_loss: 0.7185 - val_accuracy: 0.8281 - lr: 4.0000e-05\n",
            "Epoch 85/1000\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 0.1133 - accuracy: 0.9673 - val_loss: 3.2751 - val_accuracy: 0.8258 - lr: 4.0000e-05\n",
            "Epoch 86/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.0992 - accuracy: 0.9704 - val_loss: 0.7267 - val_accuracy: 0.8359 - lr: 4.0000e-05\n",
            "Epoch 87/1000\n",
            "172/172 [==============================] - 66s 386ms/step - loss: 0.0987 - accuracy: 0.9686 - val_loss: 0.7106 - val_accuracy: 0.8344 - lr: 4.0000e-05\n",
            "Epoch 88/1000\n",
            "172/172 [==============================] - 65s 376ms/step - loss: 0.1151 - accuracy: 0.9640 - val_loss: 1.3384 - val_accuracy: 0.8406 - lr: 4.0000e-05\n",
            "Epoch 89/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 0.1053 - accuracy: 0.9664 - val_loss: 0.7143 - val_accuracy: 0.8375 - lr: 4.0000e-05\n",
            "Epoch 90/1000\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.1077 - accuracy: 0.9657 - val_loss: 0.7655 - val_accuracy: 0.8172 - lr: 4.0000e-05\n",
            "Epoch 91/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.1066 - accuracy: 0.9679 - val_loss: 0.9467 - val_accuracy: 0.8352 - lr: 4.0000e-05\n",
            "Epoch 92/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.1216 - accuracy: 0.9608 - val_loss: 0.7501 - val_accuracy: 0.8336 - lr: 4.0000e-05\n",
            "Epoch 93/1000\n",
            "172/172 [==============================] - 64s 375ms/step - loss: 0.1035 - accuracy: 0.9713 - val_loss: 0.7322 - val_accuracy: 0.8422 - lr: 4.0000e-05\n",
            "Epoch 94/1000\n",
            "172/172 [==============================] - 66s 382ms/step - loss: 0.0984 - accuracy: 0.9692 - val_loss: 0.9119 - val_accuracy: 0.8234 - lr: 4.0000e-05\n",
            "Epoch 95/1000\n",
            "172/172 [==============================] - 66s 386ms/step - loss: 0.1013 - accuracy: 0.9697 - val_loss: 0.7362 - val_accuracy: 0.8461 - lr: 4.0000e-05\n",
            "Epoch 96/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.1086 - accuracy: 0.9682 - val_loss: 2.5710 - val_accuracy: 0.8328 - lr: 4.0000e-05\n",
            "Epoch 97/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.1042 - accuracy: 0.9690\n",
            "Epoch 97: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "172/172 [==============================] - 65s 377ms/step - loss: 0.1042 - accuracy: 0.9690 - val_loss: 3.5928 - val_accuracy: 0.8313 - lr: 4.0000e-05\n",
            "Epoch 98/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 0.1010 - accuracy: 0.9681 - val_loss: 3.3787 - val_accuracy: 0.8320 - lr: 8.0000e-06\n",
            "Epoch 99/1000\n",
            "172/172 [==============================] - 66s 381ms/step - loss: 0.1043 - accuracy: 0.9661 - val_loss: 0.7616 - val_accuracy: 0.8281 - lr: 8.0000e-06\n",
            "Epoch 100/1000\n",
            "172/172 [==============================] - 65s 375ms/step - loss: 0.1113 - accuracy: 0.9653 - val_loss: 0.7480 - val_accuracy: 0.8383 - lr: 8.0000e-06\n",
            "Epoch 101/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.1032 - accuracy: 0.9666 - val_loss: 0.9334 - val_accuracy: 0.8383 - lr: 8.0000e-06\n",
            "Epoch 102/1000\n",
            "172/172 [==============================] - 64s 370ms/step - loss: 0.0988 - accuracy: 0.9692 - val_loss: 0.7678 - val_accuracy: 0.8273 - lr: 8.0000e-06\n",
            "Epoch 103/1000\n",
            "172/172 [==============================] - 65s 380ms/step - loss: 0.1059 - accuracy: 0.9677 - val_loss: 0.7923 - val_accuracy: 0.8445 - lr: 8.0000e-06\n",
            "Epoch 104/1000\n",
            "172/172 [==============================] - 65s 379ms/step - loss: 0.0943 - accuracy: 0.9723 - val_loss: 2.1457 - val_accuracy: 0.8438 - lr: 8.0000e-06\n",
            "Epoch 105/1000\n",
            "172/172 [==============================] - 66s 382ms/step - loss: 0.1046 - accuracy: 0.9690 - val_loss: 1.7089 - val_accuracy: 0.8344 - lr: 8.0000e-06\n",
            "Epoch 106/1000\n",
            "172/172 [==============================] - 64s 371ms/step - loss: 0.1071 - accuracy: 0.9639 - val_loss: 0.8028 - val_accuracy: 0.8406 - lr: 8.0000e-06\n",
            "Epoch 107/1000\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9686\n",
            "Epoch 107: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "172/172 [==============================] - 64s 375ms/step - loss: 0.1044 - accuracy: 0.9686 - val_loss: 0.9687 - val_accuracy: 0.8273 - lr: 8.0000e-06\n",
            "214/214 [==============================] - 33s 153ms/step - loss: 0.8628 - accuracy: 0.9733\n",
            "Test accuracy: 0.9733138084411621\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/PokemonData'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Create data generators for training, validation, and testing with aggressive data augmentation\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False  # Ensure data isn't shuffled for evaluation\n",
        ")\n",
        "# Load VGG16 base model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Fine-tune the last few layers of the base model\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build the model with additional layers and regularization\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),  # Removed kernel_regularizer=l2(0.01)\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),  # Reduced dropout rate\n",
        "    Dense(512, activation='relu'),  # Removed kernel_regularizer=l2(0.01)\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),  # Reduced dropout rate\n",
        "    Dense(150, activation='softmax')\n",
        "])\n",
        "\n",
        "# Increasing learning rate\n",
        "model.compile(optimizer=Adam(lr=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fine-tuning more VGG16 layers\n",
        "for layer in base_model.layers[:-10]:  # Fine-tune more layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Increased patience for early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "\n",
        "# Increased factor for reducing learning rate\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=1)\n",
        "\n",
        "# Train the model with increased epochs\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    epochs=1000,  # Increased epochs\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "try:\n",
        "    test_loss, test_acc = model.evaluate(test_generator)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "except ValueError as e:\n",
        "    print(\"Error evaluating the model on the test set:\", e)\n",
        "\n",
        "# Save the model\n",
        "model.save('pokemon_classifier3.0_improved.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "model_path = 'pokemon_classifier3.0_improved.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Define image dimensions\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "class_labels_file = '/content/drive/MyDrive/PokemonData/class_labels.txt'\n",
        "\n",
        "# Read class labels from class_labels.txt file\n",
        "with open(class_labels_file, 'r') as f:\n",
        "    class_labels = f.readlines()\n",
        "    # Remove newline characters from the end of each label\n",
        "    class_labels = [label.strip() for label in class_labels]\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    # Load and preprocess the image\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (img_width, img_height))\n",
        "    img = img / 255.0  # Normalize pixel values\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    return img\n",
        "\n",
        "def predict_image(image_path):\n",
        "    # Preprocess the image\n",
        "    img = preprocess_image(image_path)\n",
        "    # Make predictions\n",
        "    predictions = model.predict(img)\n",
        "    # Get the predicted class label\n",
        "    predicted_label = class_labels[np.argmax(predictions)]\n",
        "    # Get the confidence score\n",
        "    confidence = np.max(predictions) * 100\n",
        "    return predicted_label, confidence\n",
        "\n",
        "# Test your model with images\n",
        "image_paths = ['/content/006.png']\n",
        "\n",
        "for image_path in image_paths:\n",
        "    predicted_label, confidence = predict_image(image_path)\n",
        "    print(f\"Predicted label: {predicted_label}, Confidence: {confidence:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGDjWWT1J_tR",
        "outputId": "1d46bf4e-6e61-4bb7-df32-ba0dd60a8328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 176ms/step\n",
            "Predicted label: Blastoise, Confidence: 93.50%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}